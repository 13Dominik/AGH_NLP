{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:38:57.310782Z",
     "start_time": "2024-11-01T23:38:52.051250Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "ds_dict = ds['corpus']\n",
    "ds_dict = pd.DataFrame.from_dict(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6303aa09acd21dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:38:57.366179Z",
     "start_time": "2024-11-01T23:38:57.311981Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'node-1', 'cluster_name': 'my-application-cluster', 'cluster_uuid': 'gBI4PdSoQuCa8sMPxLY-yQ', 'version': {'number': '8.15.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '98adf7bf6bb69b66ab95b761c9e5aadb0bb059a3', 'build_date': '2024-09-19T10:06:03.564235954Z', 'build_snapshot': False, 'lucene_version': '9.11.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "# Wyjatkowo nie trzeba sie autoryzowav bo w  config.yml jest to zwolnione\n",
    "es = Elasticsearch([\"http://elastics:password@localhost:9200\"], verify_certs=False)\n",
    "try:\n",
    "    resp = es.info()\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70886766ee4e710b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:38:57.696015Z",
     "start_time": "2024-11-01T23:38:57.366772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index_name = \"fiqa_index\"\n",
    "# analysis to zbior regul dla indeksu\n",
    "# analyzer to juz kolejnosc i zastosowanie ich. Elementy juz wrzcuone do maszynki ktora bedzie analizowac.\n",
    "analyzer_definition = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"polish_month_synonyms\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [\n",
    "                        \"styczeń, sty, I\",\n",
    "                        \"luty, lut, II\",\n",
    "                        \"marzec, mar, III\",\n",
    "                        \"kwiecień, kwi, IV\",\n",
    "                        \"maj, V\",\n",
    "                        \"czerwiec, cze, VI\",\n",
    "                        \"lipiec, lip, VII\",\n",
    "                        \"sierpień, sie, VIII\",\n",
    "                        \"wrzesień, wrz, IX\",\n",
    "                        \"październik, paź, X\",\n",
    "                        \"listopad, lis, XI\",\n",
    "                        \"grudzień, gru, XII\"\n",
    "                    ]\n",
    "                },\n",
    "                \"lowercase_filter\": {\n",
    "                    \"type\": \"lowercase\"\n",
    "                },\n",
    "                \"polish_morfologik_stemmer\": {\n",
    "                    \"type\": \"morfologik_stem\",\n",
    "                    \"language\": \"pl\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"synonyms_analyzer\": { # analyzer z synonimami + lematyzacja\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"polish_month_synonyms\",      # synonimy \n",
    "                        \"lowercase_filter\",           # lowercase\n",
    "                        \"polish_morfologik_stemmer\",  # lematyzajca\n",
    "                        \"lowercase_filter\"            # lowercase\n",
    "                    ]\n",
    "                },\n",
    "                \"non_synonyms_analyzer\": { # analyzer bez synonimów + lematyzacja\n",
    "                   \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase_filter\",\n",
    "                       \"polish_morfologik_stemmer\",    # Stemmer ale w rzeczywistosci on robi lematyzacje (allgero dziwnie nazwało) \n",
    "                        \"lowercase_filter\"              # Lowercase jeszcze raz bo morfologik zwraca w wielkich literach (dziwne ale tak dziala)\n",
    "                    ] \n",
    "                },\n",
    "                \"non_synonyms_analyzer_no_lemma\": { # analyzer bez synonimów i bez lematuzacji\n",
    "                   \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase_filter\",\n",
    "                        \"lowercase_filter\"             \n",
    "                    ] \n",
    "                },\n",
    "                \"non_synonyms_analyzer_non_lema\": { # analyzer z synonimów i bez lematyzacji\n",
    "                   \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase_filter\",\n",
    "                       \"polish_month_synonyms\",    \n",
    "                    ] \n",
    "                }\n",
    "\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "        \"mappings\": {\n",
    "        \"properties\": {\n",
    "            # Pole które przechowuje rekord  analizowane przez analyzer ze synonimami i z lemtyzacha\n",
    "            \"content_synon\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"synonyms_analyzer\"\n",
    "            },\n",
    "            # Pole które przechowuje rekord  analizowane przez analyzer bez synonimów i z lematyzacja\n",
    "            \"content_non_synon_lema\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"non_synonyms_analyzer\"\n",
    "            },\n",
    "            # Pole które przechowuje rekord  analizowane przez analyzer bez synonimów i bez lematyzacji\n",
    "            \"content_non_synon_non_lema\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"non_synonyms_analyzer_no_lemma\"\n",
    "            },\n",
    "            # Pole które przechowuje rekord  analizowane przez analyzer ze synoniami i bez lematyzacji\n",
    "            \"content_synon_non_lema\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"non_synonyms_analyzer_non_lema\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "#Stworzenie indexu, jesli istnieje to od nowa go\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    es.indices.create(index=index_name, body=analyzer_definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f8bb79e9c6a7ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:38:57.699267Z",
     "start_time": "2024-11-01T23:38:57.696644Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# I pakujemy dane do indexu:\n",
    "def load_data_to_es(index_name, dataset, es):\n",
    "    data = dataset['text'] # kontent dokumentu\n",
    "    ids = dataset['_id']  # id dokumentu\n",
    "\n",
    "    # bulk wrzucenie datasetu do indeksu ES\n",
    "    actions = [\n",
    "        {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc_id,  \n",
    "            \"_source\": {\n",
    "                \"content_synon\": record,\n",
    "                \"content_non_synon_non_lema\": record,\n",
    "                \"content_non_synon_lema\": record,\n",
    "                \"content_synon_non_lema\" :record\n",
    "                \n",
    "            }\n",
    "        }\n",
    "        for doc_id, record in zip(ids, data)\n",
    "    ]\n",
    "\n",
    "    helpers.bulk(es, actions)\n",
    "    print(f\"Data loaded into index '{index_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dcd939ea79aafaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:39:58.608709Z",
     "start_time": "2024-11-01T23:38:57.700470Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into index 'fiqa_index'.\n"
     ]
    }
   ],
   "source": [
    "# ładowanie bulka danych do indexu\n",
    "load_data_to_es(index_name, ds_dict, es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a4eb405dc0e2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:39:59.917790Z",
     "start_time": "2024-11-01T23:39:58.612588Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba znalezionych kwietni z synonimami: 306\n"
     ]
    }
   ],
   "source": [
    "# szablon query do ES, odowłujemy się do 'content_synon' co jesz zwiazane z jednym analyzorem, ktory ma reguly wybrane z 'analysis'\n",
    "query_synon = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_synon\": \"kwiecień\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp_synon = es.search(index=index_name, body=query_synon)\n",
    "print(\"Liczba znalezionych kwietni z synonimami:\", resp_synon['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2511b37a13b3a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:39:59.927102Z",
     "start_time": "2024-11-01T23:39:59.919608Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba znalezionych kwietni bez synonimów: 0\n"
     ]
    }
   ],
   "source": [
    "query_non_synon = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_non_synon\": \"kwiecień\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp_non_synon = es.search(index=index_name, body=query_non_synon)\n",
    "print(\"Liczba znalezionych kwietni bez synonimów:\", resp_non_synon['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945117f559182340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:39:59.935982Z",
     "start_time": "2024-11-01T23:39:59.928156Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba znalezionych listopadów z synonimami: 158\n"
     ]
    }
   ],
   "source": [
    "# powtórka eksperymentu ale dla miesiąca listopad:\n",
    "query_synon = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_synon\": \"listopad\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp_synon = es.search(index=index_name, body=query_synon)\n",
    "print(\"Liczba znalezionych listopadów z synonimami:\", resp_synon['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa5929be7aca009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:39:59.940716Z",
     "start_time": "2024-11-01T23:39:59.936534Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba znalezionych kwietni bez synonimów: 0\n"
     ]
    }
   ],
   "source": [
    "query_non_synon = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_non_synon\": \"listopad\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp_non_synon = es.search(index=index_name, body=query_non_synon)\n",
    "print(\"Liczba znalezionych kwietni bez synonimów:\", resp_non_synon['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033a3dd5241e681",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "ES działa, zarówna dla kwietnia jak i listopada w przypadku z synonimami znalazł więcej rekordów - zgodnie z intuicją."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aeb251575a30a3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad8e50a15f0dae80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# NDCG@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c4372c578a24ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:02.811291Z",
     "start_time": "2024-11-01T23:39:59.941280Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>566392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>65404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>325273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>88124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>285255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>11039</td>\n",
       "      <td>330058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>11039</td>\n",
       "      <td>91183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>11054</td>\n",
       "      <td>155053</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>11054</td>\n",
       "      <td>321015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>11088</td>\n",
       "      <td>437100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1706 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query-id  corpus-id  score\n",
       "0            8     566392      1\n",
       "1            8      65404      1\n",
       "2           15     325273      1\n",
       "3           18      88124      1\n",
       "4           26     285255      1\n",
       "...        ...        ...    ...\n",
       "1701     11039     330058      1\n",
       "1702     11039      91183      1\n",
       "1703     11054     155053      1\n",
       "1704     11054     321015      1\n",
       "1705     11088     437100      1\n",
       "\n",
       "[1706 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QA dataset \n",
    "ds_qa = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "data_qa = ds_qa['test']\n",
    "data_qa[0]\n",
    "data_qa = pd.DataFrame(data_qa)\n",
    "data_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30be06826dd1b7f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:02.817473Z",
     "start_time": "2024-11-01T23:40:02.813736Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_qa['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb84ce1ed11a8dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Warto zauwazyc ze dla kazdje pary q-a w datasecie jest wartosc score = 1. Wiec wyniki moga wyjsc dosc dziwne bo w tym przypadku mamy binarne dopasowanie pasuje/nie pasuje. Nie wykorzystujemy tutaj pelnego potencjalu ndcg gdzie mozna wrzucac wazone odpowiedzi ktore pasuja lepiej lub gorzej do query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12afc027933c1c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:05.321815Z",
     "start_time": "2024-11-01T23:40:02.818083Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# queries data set zawiera query\n",
    "data_queries = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")\n",
    "df_queries = pd.DataFrame(data_queries['queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "193af602b381b934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:05.325470Z",
     "start_time": "2024-11-01T23:40:05.322463Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id                                                    915\n",
       "title                                                     \n",
       "text     Maksymalizacja HSA po maksymalnym wykorzystani...\n",
       "Name: 420, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries.iloc[420]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e5d3701e67d68f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:05.331043Z",
     "start_time": "2024-11-01T23:40:05.327522Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_dcg(scores):\n",
    "    return sum(score / np.log(idx + 2) for idx, score in enumerate(scores))\n",
    "\n",
    "\n",
    "def compute_ndcg(relevant_scores, retrieved_scores, k=5):\n",
    "    dcg = compute_dcg(retrieved_scores[:k]) # to sa te ktore zwrocil nasz 'model'\n",
    "    ideal_dcg = compute_dcg(sorted(relevant_scores, reverse=True)[:k]) # relevant uzywamy do idealnego dcg (idealne ulozenie odpowiedzi)\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "NDCG_SIZE = 5\n",
    "\n",
    "\n",
    "def search_and_compute_ndcg(index_name, analyzator_content, test_data, ndcg_size):\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    # obliczamy dla kazdej query dostepnej w testowym zbiorze danych\n",
    "    for index, row in test_data.iterrows():\n",
    "        # query id\n",
    "        query_id = row[\"query-id\"]\n",
    "        # query text\n",
    "        query = df_queries[df_queries['_id'] == str(query_id)]['text'].values[0]\n",
    "        # Wykonanie zapytania do Elasticsearch\n",
    "        search_query = {\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    analyzator_content: query,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # bierzemy 5 pierwszych dopasowań od Elastic search (dostał query) zwraca nam 5 dokumentów\n",
    "        response = es.search(index=index_name, body=search_query, size=ndcg_size)\n",
    "        retrieved_docs = [hit[\"_id\"] for hit in response[\"hits\"][\"hits\"]] # id 5 dokumentow zwrocone przez ES\n",
    "        # Wszysktie A które pasuja do Q (z labelowanego dataset)\n",
    "        good_answers = data_qa[data_qa['query-id'] == int(query_id)]\n",
    "        # sortuje je po ich 'score', one i tak mają 1 ale na przyszlosc z lepszym datasetem zeby gralo bo tak sie realizuje IDCG\n",
    "        good_answers = good_answers.sort_values(by='score', ascending=False)\n",
    "        # Biore posortowane kolejne elementy z dobrymyim odpowiedziami, jesli nie ma ich (5) to uzupelniam 0 ami aby było zawsze 5 elementów - prawidlowe ndcg tak działa\n",
    "        relenvant_answears = list(good_answers['score'][:ndcg_size]) + [0] * (ndcg_size - len(good_answers)) # idealne odpowiedzi\n",
    "        # print(relenvant_answears) -> cos w stylu [1,1,0,0,0]\n",
    "        \n",
    "        \n",
    "        retrived_answears = [0 for _ in range(ndcg_size)] #otrzymane odpowiedzi\n",
    "        for idx, doc_found in enumerate(retrieved_docs):\n",
    "            if int(doc_found) in good_answers['corpus-id'].values:\n",
    "                retrived_answears[idx] = good_answers[good_answers['corpus-id'] == int(doc_found)]['score'].iloc[0]\n",
    "        \n",
    "        #print(retrived_answears) # -> cos w stylu [0,1,0,0,0]\n",
    "        ndcg = compute_ndcg(relenvant_answears, retrived_answears, k=5)\n",
    "        ndcg_scores.append(ndcg) # ndcg dla kazdego query sumujemy\n",
    "\n",
    "    # Zwracamy średnie NDCG dla wszystkich zapytań\n",
    "    return np.mean(ndcg_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c8ab032069902a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:05.333611Z",
     "start_time": "2024-11-01T23:40:05.331614Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "qa_no_duplicates = data_qa.drop_duplicates(subset='query-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41f858b8d20a8c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:09.417706Z",
     "start_time": "2024-11-01T23:40:05.334105Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnie NDCG dla QA (synonimy + lematyzacja): 0.1851291130797741\n"
     ]
    }
   ],
   "source": [
    "mean_no_synonyms = search_and_compute_ndcg(index_name, 'content_synon', qa_no_duplicates, NDCG_SIZE)\n",
    "print(\"Średnie NDCG dla QA (synonimy + lematyzacja):\",mean_no_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8302afdf115eee3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:13.222208Z",
     "start_time": "2024-11-01T23:40:09.418728Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnie NDCG dla QA (brak synonimy + lematyzacja): 0.1851291130797741\n"
     ]
    }
   ],
   "source": [
    "mean_no_synonyms = search_and_compute_ndcg(index_name, 'content_non_synon_lema', qa_no_duplicates, NDCG_SIZE)\n",
    "print(\"Średnie NDCG dla QA (brak synonimy + lematyzacja):\",mean_no_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e60b044f9efb811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:16.202002Z",
     "start_time": "2024-11-01T23:40:13.223221Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnie NDCG dla QA (brak synonimy + brak lematyzacja): 0.13854570378524392\n"
     ]
    }
   ],
   "source": [
    "mean_no_synonyms = search_and_compute_ndcg(index_name, 'content_non_synon_non_lema', qa_no_duplicates, NDCG_SIZE)\n",
    "print(\"Średnie NDCG dla QA (brak synonimy + brak lematyzacja):\",mean_no_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87e0cb456d90f871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T23:40:20.082669Z",
     "start_time": "2024-11-01T23:40:16.202571Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnie NDCG dla QA (synonimy + brak lematyzacja): 0.13839287982649878\n"
     ]
    }
   ],
   "source": [
    "mean_no_synonyms = search_and_compute_ndcg(index_name, 'content_synon_non_lema', qa_no_duplicates, NDCG_SIZE)\n",
    "print(\"Średnie NDCG dla QA (synonimy + brak lematyzacja):\",mean_no_synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8946128ff4f94",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Odpowiedzi na pytania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd494f5f9d0f980a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1 - regex vs ES:\n",
    "Generalnie uważam, że dużo większe pole do popisu daje ES, jednak poziom wejscia jest zancznie wiekszy. Rgexy to import biblioteki w jednej linijce i mozna dzialac, jest masa gotowych regexow, GPT dobrze w miare sobie z nimi radzi. Zestawienie ES silnika, ogarniecie w dokumentacji itd to roboty znacznie wiecej. Tutaj bylo na dockerze to poszlo szybko, bez tego to kolejne kilka gdozin. Jednak ES daje znacznie bardziej 'flexible' mozliwosci. Jeli zalezy nam na zakodzeniu czegos na szybko (prototpowaniu) i cos co ma sztywno okreslona struktue - na przykald kod pcoztowy -> ja bym wybieral RE. Jednak bardziej skomplikowane zastosowania typu dopasowanie produktu do wyszukiwania jak robi to Allego - ES warto. Proste problemy -> RE, skomplikowane -> ES. Oba bardzo fajne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e0a994a534e21",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2 - LLM z pewnoscia moze byc stosowany (w sumie to do czego akutalnie nie moze byc :) ). Jendak tutaj operujemy na prawdopodobienstwie co za tym idzie wydaje mi sie ze wieksza szansa ze wyprowadzi nas w maliny niz RE czy ES. Ponadto moze halucynowac i zwracac kompletnie oderwane dokumenty. Intuicyjnie bedize on zwracal wiecej False positive jednak moze zrozumiec nieraz lepiej kontekst niz klasyczne ES. Watyo jednak rozwazyc te opcje i dopasowac ja na potrzeby taska. Mozna kombinowac z jakimis pdosumowaniami i szukac w tym, no LLM daja wielkie pole manewru, jednal nei zdawalbym sie w pelni na nie w 100%. Nieraz klasyczne metody sa wsytarczajace i latwiejsze w kontrolowaniu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628889dfd16e7df0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Wnioski"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104265f8e101ea4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "ćwiczenie - fajne. Pierwsz okazja do zapozaniani się z Elastic Search, kiedyś cos się obiło o uszy jednak nic szczególne. Tutaj bardzo ciekawa opcja na przetestowanie na cyzmś praktycznym. Dobrze można zobaczyć róźnice w wykorzystaniu regexów a ES. Tutaj poziom skomplikowania znacznie przebija, jednak ES dopasuje nam dużo więcęj różnych odmian, fleksji i przypadków. Zestawienie różnych opcji co do tokenizera lematyzacji itd daje odrazu różnice we wyniku i uświadamia o jak wielu rzexZch trzeba pamiętać robiąc NLP. Zadanie z pozoru bardzo proste, bo wyszukanie słowa kwiecień w tekscie, brzmi jak 1 rok studiów. W prktyce zabiera cały dzień pracy na ostantim roku. Jednak oceniam dobrze doświadczenie. Można było zrobić coś ciekawego zobaczyć jak działa na dobrze dobranym przykladzie\n",
    "NDCG - metryka dla mnie nowa, zapoznałem się dopiero z nią na wykładzie. Ciekawiło mnie zawsze jak bardziej skomplikowane taski niz klasyfikacja sa walidowane tutaj QA - już wiem. Metryka dość intuicyjna prosta do zrozumeinai, łatwo implementuje się widząc przed oczami slajd z wykładu z obrazkime gdzie jest przykład. moze by stosowana do wielu zadan, jak sie okazuje nawet do metod QA bez trenowania modeli. Czy wynik na poziomie 0.18 jest dobry? Ciezko powiedziec, penwie nie, trzbea by porownac  zinnymi bardziej zaawansowanymi technikami. Dobry punkt wyjściowy to może być.\n",
    "\n",
    "Wyniki w ćwiczeniu - dośc imtuicyjne dodawanie synonimów/lematyzacji poprawia dopasowania miesięcy i poprawia wynik NDCG. Dobry obrazowy przykład."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11652797ea483b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
