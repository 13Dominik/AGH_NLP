{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:06.081738Z",
     "start_time": "2025-01-20T19:08:04.638960Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import Qdrant, QdrantVectorStore\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955cbc86a8b7102",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Ollama settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff661ca55b02f8dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:06.242690Z",
     "start_time": "2025-01-20T19:08:06.082510Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED    \r\n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    4 weeks ago    \r\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0a99bb9a8a158f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:06.370682Z",
     "start_time": "2025-01-20T19:08:06.244652Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\r\n"
     ]
    }
   ],
   "source": [
    "!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a4673a1a17dd6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:11.760232Z",
     "start_time": "2025-01-20T19:08:06.372244Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tak, mówię po polsku. Jak mogę pomoć Ci w języku polskim?\n"
     ]
    }
   ],
   "source": [
    "def query_llama(prompt, model=\"llama3.2\"):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Błąd {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    full_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                json_line = line.decode('utf-8')\n",
    "                data = requests.models.complexjson.loads(json_line)\n",
    "                full_response += data.get(\"message\", {}).get(\"content\", \"\")\n",
    "            except ValueError:\n",
    "                print(f\"Niepoprawny JSON: {line}\")\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "prompt = \"Mówisz po polsku?\"\n",
    "result = query_llama(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f76d99bf9923df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed1193cbd25dbda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:11.767868Z",
     "start_time": "2025-01-20T19:08:11.762225Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !docker pull qdrant/qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f19f88457422bf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:11.772627Z",
     "start_time": "2025-01-20T19:08:11.769450Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !docker run -p 6333:6333 -p 6334:6334 \\\n",
    "#     -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "#     qdrant/qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37f61bd6ca6c55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:11.834211Z",
     "start_time": "2025-01-20T19:08:11.773525Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebe48c57b7325c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1410a1cfd564bb32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:12.044771Z",
     "start_time": "2025-01-20T19:08:11.834978Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "doc = load_pdf(\"./doc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4392d4fcc902729b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:12.049730Z",
     "start_time": "2025-01-20T19:08:12.045772Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Split documents into appropriate chunks\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    return split_docs\n",
    "chunks = split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb425fe37b1fbe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:12.054958Z",
     "start_time": "2025-01-20T19:08:12.051891Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af03c70b98d43208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:17.281687Z",
     "start_time": "2025-01-20T19:08:12.055565Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/NLP/lib/python3.10/site-packages/sentence_transformers/models/Dense.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Load model to generate embeddings:\n",
    "model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/sentence-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b01d531449319b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:17.511432Z",
     "start_time": "2025-01-20T19:08:17.282830Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size:: 768\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = model.embed_query('test')\n",
    "print(f\"Embedding size:: {len(embedding_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3868a9f2eeb533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:17.949814Z",
     "start_time": "2025-01-20T19:08:17.512125Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "# Create proper collection\n",
    "\n",
    "# Delete if already exist\n",
    "client.delete_collection(collection_name=\"rag_collection\")\n",
    "\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"rag_collection\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569ed72bd7480220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:20.515422Z",
     "start_time": "2025-01-20T19:08:17.950692Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bc64f923ec984597bced92874f7d0761',\n",
       " 'a97093b1a30b42d9b82bcad64d5f534d',\n",
       " '19d8cfbd0efb431e8217ef78e38460ab',\n",
       " '5dc231b7e7ab49c4b06e208fe68c7cf1',\n",
       " 'b360b3775bb547dbb962e6c4ad0cab77',\n",
       " 'f2352a13f8b64d00bb47ba26403297b8',\n",
       " 'df68ebf9fa4d4272bf4a21337f7a305e',\n",
       " 'e1b0ea7e3fa540be90e0081ebaaad595',\n",
       " '372eceaab0534e2e97aa33b6f2eba944',\n",
       " '021508b846f84f919b6a92b04300bfc5',\n",
       " '8be295123f12458a9c64aa2e16986bd4',\n",
       " 'f754dc36ea6e4f8c86656c9fc2544403',\n",
       " '226ecaf9b3d048109693e776fec79e00',\n",
       " '84301e760f504d0f933c468c92f65311',\n",
       " '2796a2b50ea546a18b09e373fcf3487a',\n",
       " '900c22c781474645aa80c7d6a506f5b8',\n",
       " 'd0e76c649f904b05acab5ec3e9b8f233',\n",
       " '3e96a59e619a41e68a7f1dba9ca19e24',\n",
       " '6ae29908fac64d10af01922778d07cdf',\n",
       " 'b5717c35e65e4629913dde2ee13551b7',\n",
       " 'f9163ffbd01142f3b64fb89b0f085e05',\n",
       " '5075d9a0976a4f35b7ce86cef5cf9a23',\n",
       " '6cf380ce31c8467084c7f984cc614f4e',\n",
       " '5e16e2b3234242fc8987872a980df153',\n",
       " '9174c31cad4049619ac4cbe765a59d85',\n",
       " 'a49971d8052f42f9a852abf587d01eca',\n",
       " '740468caab7c4abcaf2a9a0c01e8ab29',\n",
       " '3c8faec717824b22b04949da73a98a93',\n",
       " '1e40751e8d894a7b92e60a0e0557ed11',\n",
       " 'd8d2f134cee6473f96c2ff830916f757',\n",
       " '40a73ea0c19c420b8d064c1eaa9a3def',\n",
       " '64c1d31373e3457e988add3f8a07aa90',\n",
       " '0fb5d4ed4af44904974be9bce34a650b',\n",
       " '1baf12e19a624ac4afc3d5ba73ec1b00',\n",
       " '75f0a3e1c28241ca9b9dc9114e6c4998',\n",
       " '06680d618e6c45e6a5c9226cc172e37d',\n",
       " '9eebeefdaecd4ed8be56aebe58939df2',\n",
       " '581624cf395d4e9eb18e3166b33d239a',\n",
       " 'edd548bdb22e4946ab0d322c2a8c6840',\n",
       " '5bfef9e461bb4d93808dc7a49c84f628',\n",
       " 'a3a1f4e1379049459cfb6ebe9c11c59f',\n",
       " 'fe933375a2ca435cacd8910adc8df5f5',\n",
       " '41f1af91c143492faae790c15cb9e37b',\n",
       " 'fa11c5fdc40e4a63943d5fd2e6993f0a',\n",
       " 'c7f7e8bffc1541358b0b2fadae7010c6',\n",
       " '85bab6f7e9f3478ea8997dbfc7c947f4',\n",
       " 'a5bcee6e812c4871af00226eed2859fa',\n",
       " 'c3de2bbeeba04ad88dcfcfaa6ed59ff3',\n",
       " 'ec4ae863843a460684039e79ade9c231',\n",
       " 'f19ae663754345c18f1057eb7c457800',\n",
       " '29ad92a104f1436a930435d675d55bc7',\n",
       " '2edf9e42d0be44348ed3e6524ca77b08',\n",
       " '6d5ccbe375e34422bfc1130963954ef3',\n",
       " 'bfe5c90ca4534c52994c397a16f1b073',\n",
       " '9b26e02fcd4c416ba5bb1a3db7709e0d',\n",
       " '017a55c1b34248ca9c5dc711293ed299',\n",
       " '69b7e05527be4970b71f54aa2cc013bb',\n",
       " '2fba138ba78f46d287b5cbfa612ea477',\n",
       " '06fac3862510427e8c277811e154286f',\n",
       " '1e1017011a2c4ad1bafec4d4673e4e4e',\n",
       " 'f6ca5a8d72b94992946eaaa17ec4bc93',\n",
       " 'fac4d95e81154be48b6399d002935646',\n",
       " '497bf26740024877b89c4522645c3644',\n",
       " '34eaf08708a64d9d8a711de31851566e',\n",
       " '0b3beb86fa86470da0b80b103f24ed33',\n",
       " '7208ea2df72a4d6faca2663a48a2879f',\n",
       " '23f7ab43206849eca2d68ba43f3b4471',\n",
       " '82c38054221440ac98c6b96d76b0a4bf',\n",
       " '93f375072c8d4aaab438ac611f5df4fd',\n",
       " '8c969ec6749e4b248c6085d015d66429',\n",
       " 'c5eacd99390d4dd38c51becb633e7626',\n",
       " '748e495ac3d8482cb74db69db66591e3',\n",
       " '8e6310b50e3547ababb106f91ed24b27',\n",
       " 'b02076b625b64ce398caec1f7badf586']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Initialize Qdrant client\n",
    "qdrant = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name='rag_collection',\n",
    "        embedding=model\n",
    "    )\n",
    "\n",
    " # Push documents and their embeddings to Qdrant\n",
    "qdrant.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1f5f54f754de65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:20.532232Z",
     "start_time": "2025-01-20T19:08:20.516032Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: \n",
      " status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=74 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=768, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfig(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, filter_max_conditions=None, condition_max_size=None)) payload_schema={}\n",
      "\n",
      "\n",
      "Number of records: 74\n",
      "\n",
      "\n",
      "First record:\n",
      " ([Record(id='017a55c1-b342-48ca-9c5d-c711293ed299', payload={'page_content': '2019. [Online]. Available: http://arxiv.org/abs/1904.10117\\n[34] U. Bhattacharya, T. Mittal, R. Chandra, T. Randhavane, A. Bera, and\\nD. Manocha, “STEP: spatial temporal graph convolutional networks\\nfor emotion perception from gaits,” CoRR, vol. abs/1910.12906, 2019.\\n[Online]. Available: http://arxiv.org/abs/1910.12906\\n[35] A. Luo, F. Yang, X. Li, D. Nie, Z. Jiao, S. Zhou, and H. Cheng,\\n“Hybrid graph neural networks for crowd counting,” CoRR, vol.\\nabs/2002.00092, 2020. [Online]. Available: https://arxiv.org/abs/2002.\\n00092\\n[36] Y . Fan, J. C. K. Lam, and V . O. K. Li, “Facial action unit intensity\\nestimation via semantic correspondence learning with dynamic graph\\nconvolution,” CoRR, vol. abs/2004.09681, 2020. [Online]. Available:\\nhttps://arxiv.org/abs/2004.09681\\n[37] Y . Zhou, S. Graham, N. Alemi Koohbanani, M. Shaban, P.-A. Heng,\\nand N. Rajpoot, “Cgc-net: Cell graph convolutional network for grading\\nof colorectal cancer histology images,” in 2019 IEEE/CVF Interna-', 'metadata': {'source': './doc.pdf', 'page': 7}}, vector=[-0.015875349, -0.0069654877, 0.0461742, 0.0054711695, 0.00016135209, 0.0107703535, -0.048444115, 0.05240123, 0.007339607, -0.029537108, 0.050810765, -0.07120377, 0.116572544, -0.0016317817, 0.0813068, 0.023122724, 0.020690223, -0.008301584, 0.012651688, 0.018872818, 0.016068984, -0.03884263, -0.063274935, 0.022856835, -0.06607594, 0.019632552, 0.021943567, 0.01122863, -0.019135304, 0.011388889, 0.060201168, -0.020503305, -0.04304001, 0.049993236, -0.061210148, 0.024743667, 0.008676004, -0.0545437, -0.020477533, 0.034833673, 0.015362387, -0.061883744, -0.05360237, 0.043987013, -0.028250355, -0.018164197, 0.029754592, 0.051029492, -0.0016286252, -0.08320966, -0.058969118, -0.03968604, -0.039203, -0.020535043, -0.008924127, -0.009193761, -0.026751332, -0.004591143, 0.0061982637, 0.00795342, -0.03986046, 0.03471324, -0.04757855, -0.009913872, -0.0320882, 0.026037788, -0.049670134, -0.06353247, 0.046882767, -0.031494673, 0.032392044, 0.017654702, -0.043736253, -0.009342252, -0.020722969, -0.02788869, -0.06834567, 0.0018308054, 0.06848, -0.049263064, 0.06541355, -0.0020943738, 0.007860735, 0.0070873876, 0.009578401, -0.026946861, -0.037892044, 0.02669638, -0.033884685, 0.023539493, 0.015914569, -0.034735292, -0.00065352436, 0.013899195, -0.07415845, 0.06627359, 0.009669093, -0.013158717, -0.030821169, 0.008802345, -0.014871479, -0.01763313, -0.021296024, -0.0061079455, 0.07162486, -0.03730274, -0.047811482, -0.022285964, -0.030021992, -0.0163918, -0.024557294, -0.027775597, -0.03501971, 0.036507368, 0.04779124, -0.01421915, -0.0077151144, -0.014766753, -0.010150031, 0.006587966, -0.016480902, -0.004003078, -0.07849095, -0.024255408, 0.05108948, -0.008238767, -0.022361444, -0.041967556, -0.04754459, 0.10378444, -0.0031132367, 0.03105559, 0.048165575, -0.02602168, 0.008658781, 0.0083572315, 0.0043912153, 0.03562192, -0.024378993, 0.010614449, 0.013782624, 0.017664745, 0.038622387, -0.02190525, -0.013776647, 0.013403805, -0.03581479, -0.021922447, 0.0645099, 0.02941584, -0.014281116, 0.017276851, 0.041788775, 0.062300332, 0.015224476, -0.0070114858, -0.004960038, -0.01987909, -0.056649093, 0.008377732, -0.049178094, -0.0036284868, 0.048798196, 0.03268421, 0.0045265784, -0.026127715, -0.014203137, -0.017631885, -0.036981296, -0.028809471, -0.022961508, -0.01853422, 3.019765e-05, -0.03215218, 0.027540164, 0.008294113, 0.01928031, -0.030776933, 0.0061175483, -0.028217986, 0.013612858, 0.025693234, -0.0020476785, 0.047104426, 0.0023839471, 0.024936892, 0.009055233, 0.04461154, 0.0033863126, -0.0564535, 0.015484473, 0.018293908, -0.0010947088, 0.054778393, 0.029788611, 0.031143565, -0.0005094909, -0.03814347, -0.061274182, 0.012845203, -0.05816141, -0.0019476862, 0.0020678984, 0.015512321, -0.028618326, 0.011050829, 0.017592004, 0.006287942, -0.029940134, -0.01767538, 0.07935565, -0.007584665, 0.0072410335, 0.019383725, 0.024154868, -0.016495792, 0.019890709, -0.014327777, 0.040088166, 0.0114134755, -0.033506602, 0.09484162, -0.002022242, 0.005777109, -0.037798706, -0.045626603, -0.06668996, -0.037806105, 0.014796032, 0.03353316, 0.047501482, 0.06609188, 0.055188216, -0.020706918, -0.024950577, -0.028681144, -0.034457862, -0.020033088, -0.023144655, -0.019663544, 0.020030497, 7.096875e-05, 0.029364217, 0.0070308098, 0.053411815, -0.022600496, 0.0053141704, 0.03798636, 0.012481911, 0.05042274, -0.027404716, 0.02946103, 0.03527828, 0.009767591, 0.028955728, -0.03203908, 0.004834198, 0.048745256, 0.011283859, -0.043613665, -0.06320746, 0.037129514, 0.013410505, 0.054033324, 0.0006229879, -0.03363942, 0.032810315, 0.036718294, 0.023162307, -0.005847344, 0.031916272, -0.016967222, -0.029515294, -0.034324262, 0.005251595, 0.053998932, 0.02129304, 0.048292767, 0.020015672, -0.06373734, -0.012287566, 0.01532743, 0.0062953895, 0.010303083, 0.00014788425, -0.011621836, -0.0042973845, 0.04381625, -0.04330308, -0.059061985, -0.01288806, -0.045611933, 0.013766641, 0.0698427, 0.010001532, 0.029394882, 0.031084232, 0.0043004416, -0.008174272, -0.07645016, -0.0020247935, -0.032814484, 0.029010143, 0.05681222, 0.03417121, -0.014806777, 0.03071607, 0.01256157, 0.0061435676, 0.0040447167, -0.024285821, -0.027666928, -0.053001504, -0.12320823, 0.0072593032, -0.01104832, 0.043057308, -0.02295166, -0.02239738, 0.027170623, 0.04010457, 0.020499974, 0.03688874, -0.024222488, -0.004642608, -0.053181108, 0.03395161, -0.08429825, -0.019062253, 0.00752572, -0.007552835, -0.02111269, 0.012634743, -0.00080931693, -0.06515278, -0.04199769, 0.016244804, -0.047311243, 0.063809656, -0.02004981, 0.019479165, -0.021830244, -0.038631484, 0.027428932, 0.017095175, -0.029227944, -0.019203538, -0.04217614, -0.007358898, 0.036786456, -0.06069248, 0.035436258, 0.0019546347, -0.021076998, -0.027183224, 0.038481493, -0.044893265, 0.047347903, -0.008736503, -0.0050859153, 0.026396964, -0.051742338, -0.023172745, -0.044838987, -0.024970176, 0.028977558, 0.051775772, 0.0030849783, 0.008020074, -0.015016061, 0.022923598, 0.025397124, 0.029175341, 0.06383965, 0.04943039, 0.07402747, 0.023235051, 0.028022248, 0.005321247, 0.03919142, 0.049711984, 0.0047092186, -0.047381714, 0.017644785, 0.02280464, -0.020726712, 0.016554475, 0.008134878, -0.055281702, -0.116545625, 0.04875338, -0.028426496, -0.017954927, 0.030817922, 0.015607002, -0.011244873, 0.043933388, -0.003092373, -0.071185574, 0.062448427, -0.03936368, 0.033439167, -0.05103002, 0.050892517, -0.044770278, 0.015801413, 0.018355997, 0.018293604, 0.03809522, 0.020202827, -0.013609132, -0.019532122, -0.026924044, -0.020432029, -0.012283559, 0.019931238, 0.02023957, -0.07772872, -0.01813818, 0.019949188, 0.044698272, -0.029250946, 0.051351555, 0.05573416, 0.027882421, 0.018472986, 0.0034021095, 0.012590669, 0.061325874, -0.059580274, 0.056243345, -0.015419596, -0.021186622, -0.0010060379, 0.0028125884, 0.005707207, -0.0019004901, -0.025639879, 0.030294975, 0.041502662, -0.03233616, -0.038026717, 0.016551036, 0.0629802, 0.06205895, 0.040279437, 0.019560276, -0.006651626, 0.027225187, -0.011352207, -0.03528585, 0.012051394, 0.009929598, -0.028253566, 0.034478847, -0.009827094, -0.0011952185, 0.08200636, 0.014289647, -0.009449574, -0.028443972, 0.0022893273, -0.03437391, 0.048654284, 0.025345508, 0.0043436512, 0.09585258, -0.042542066, -0.0044853305, 0.039919063, -0.02150709, -0.009763108, -0.049827438, 0.055897422, -0.05449576, 0.0068352334, -0.030967148, 0.016247826, 0.04417479, -0.020722903, 0.013517953, 0.003992453, -0.015377069, -0.006114429, 0.017068649, 0.06332138, 0.027384065, -0.009035345, -0.057557303, -0.096217774, -0.016807588, -0.047766984, -0.09212246, 0.0151554495, -0.047434058, -0.071751416, 0.041076988, -0.04060379, 0.027507456, 0.031652953, -0.046676386, -0.01279196, -0.01462316, 0.0023056616, 0.0053017233, -0.040932354, 0.063395284, -0.08523421, -0.005725372, -0.0006754413, -0.0016520881, 0.03114992, 0.046565212, 0.022464033, 0.0031518058, -0.07698077, 0.008409945, -0.00911317, 0.030702768, -0.050052315, 0.030846523, -0.029336963, -0.015426935, -0.00055759493, 0.0007252214, 0.023578063, -0.054296203, 0.036007676, 0.0013034326, 0.025177116, -0.020680832, 0.016681993, -0.0036926644, -0.004819815, -0.017937236, 0.038145196, 0.03973419, -0.002356332, 0.00415787, 0.067465864, -0.048433576, 0.032467246, 0.05174455, -0.010880592, -0.022317654, -0.0030954191, -0.06389456, 0.0076108407, 0.08294653, 0.043627605, -0.031565253, 0.011786188, -0.02161758, 0.03276425, 0.024287399, -0.06058156, 0.0053186878, 0.076401405, -0.015496603, -0.025640752, 0.038752757, 0.0035720726, -0.032538153, 0.0042452, -0.06582081, -0.032460716, 0.02833362, 0.06857006, 0.03637221, 0.021316214, -0.04858013, -0.07788803, -0.05628649, -0.068657294, -0.019579532, -0.00940176, 0.0018093422, 0.008236779, 0.012759516, 0.031467214, -0.020253994, -0.023194047, -0.08370603, -0.036749832, 0.009677339, -0.023828259, 0.07090329, 0.02498597, -0.024006488, 0.05522564, 0.0011068038, 0.0178025, 0.0018036491, -0.013041798, 0.064903125, -0.029214302, 0.022747729, -0.0065339627, 0.015638854, 0.03924202, 0.033830654, 0.032928765, 0.0053220484, -0.018605642, -0.004172499, 0.008532401, 0.024573248, 0.019621937, -0.017264158, -0.02693359, 0.037475973, -0.09242806, -0.008394165, 0.008669328, 0.013730509, -0.04430446, -0.041824747, 0.024157258, 0.024936682, -0.061275177, 0.010538368, 0.04342917, -0.0317566, 0.0006085312, 0.036808673, -0.046212867, -0.014739304, 0.032829214, -0.070102625, 0.033942584, -0.027734568, -0.009224877, 0.067004964, 0.03078212, 0.00030101548, 0.026059076, 0.052155916, 0.07585743, 0.0019584196, 0.044708878, 0.021213382, 0.0060396977, -0.016036427, -0.022673767, 0.013779585, 0.0026184598, -0.007451416, 0.013658312, 0.07650168, -0.04446848, -0.013263999, 0.00986506, 0.0448496, 0.051043227, 0.021581257, -0.022317547, -0.05028097, -0.07117277, -0.024328822, 0.02314497, 0.02215658, 0.018721808, -0.019952066, 0.027740013, 0.061906002, -0.03339095, -0.011677911, 0.006742808, -0.031201154, -0.013539258, 0.027059276, -0.0089713335, 0.010472734, 0.043059174, 0.022308212, -0.005370809, -0.1057458, 0.038525477, -0.011168202, 0.030959422, -0.029347105, 0.05038519, -0.0042596846, -0.012408922, 0.017218703, 0.020779228, 0.03841458, 0.009950262, -0.005993681, -0.04217211, -0.013177255, -0.0148253385, 0.005735626, -0.03297975, -0.01023205, -0.012502658, -0.047215603, 0.039736778, 0.034157876, -0.017336592, -0.0374253, 0.04861482, -0.011396373, -0.023336012, 0.0017427828, -0.039233852, 0.048456393, 0.02155243, -0.06411867, 0.07534804, 0.05723908, 0.024731286, 0.010747568, -0.008862013, 0.0060009956, -0.030940447, 0.0025170033, 0.016558828, -0.07338678, -0.061087374, -0.000581736, 0.011236207, 0.03928276, 0.0034576163, 0.019230261, 0.046191487, 0.015513466, -0.032948315, -0.025327317, 0.015240656, -0.011136517, -0.015218348, -0.036621112, 0.019218074, 0.008280841, -0.0012318154, -0.054657202, -0.012706145, -0.041901983, 0.034391847, -0.00041411413, -0.060485225, 0.011552803, -0.005998331, -0.013528659, 0.019423438, -0.06807667, -0.016989045, 0.02681885, -0.004593069, -0.009597498, 0.05197205, 0.091451585, -0.03921584, -0.027966017, -0.012766084, -0.0406215, -0.0015547569, -0.02150983, -0.022070194, -0.025451263, -0.05096562, 0.00868577, 0.011293597, 0.055876978, 0.02079349, 0.009081132, 0.0054072724], shard_key=None, order_value=None)], '021508b8-46f8-4f91-9b6a-92b04300bfc5')\n"
     ]
    }
   ],
   "source": [
    "collection_info = client.get_collection('rag_collection')\n",
    "print(f\"Info: \\n {collection_info}\")\n",
    "print(\"\\n\")\n",
    "stats = client.get_collection('rag_collection').points_count\n",
    "print(f\"Number of records: {stats}\")\n",
    "\n",
    "query_result = client.scroll(\n",
    "    collection_name='rag_collection',\n",
    "    limit=1,\n",
    "    with_vectors=True,\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(f\"First record:\\n {query_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef7e77ef50f29d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d6c21e2592f71be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:20.564296Z",
     "start_time": "2025-01-20T19:08:20.532928Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever()\n",
    "llm = OllamaLLM(model='llama3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7b6aaddfb528a72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:29.268800Z",
     "start_time": "2025-01-20T19:08:20.564929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the capital of France?', 'result': 'I don\\'t know how to answer this question based on the provided context about Graph Neural Networks (GNNs) in Computer Vision. The text appears to be a collection of research papers and references related to GNNs applications, but it does not contain any information about geography or answering questions like \"What is the capital of France?\".'}\n"
     ]
    }
   ],
   "source": [
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# Run RAG pipeline\n",
    "query = \"What is the capital of France?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf6b0f5188bc6793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:29.276852Z",
     "start_time": "2025-01-20T19:08:29.270792Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def query_rag(rag_chain, query):\n",
    "    return rag_chain.invoke(query)['result']\n",
    "def compare_rag_llm(query):\n",
    "    rag_result  = query_rag(rag_chain, query)\n",
    "    llm_result = query_llama(query)\n",
    "    \n",
    "    print(\"Answer RAG:\")\n",
    "    print(rag_result)\n",
    "    print(\"Answer LLM:\")\n",
    "    print(llm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbcb1edae3f2d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:38.610520Z",
     "start_time": "2025-01-20T19:08:29.278249Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer RAG:\n",
      "According to the context, Graph Convolutional Networks (GCNs) and other graph neural network models can work directly with graphs and their structural information.\n",
      "Answer LLM:\n",
      "Graph Neural Networks (GNNs) typically take the following inputs:\n",
      "\n",
      "1. **Adjacency Matrix**: The adjacency matrix of the graph, which represents the connections between nodes in the graph. In this matrix, each entry represents the weight or strength of the connection between two nodes.\n",
      "2. **Node Features**: One or more vectors representing the attributes or features of each node in the graph. These features can be categorical, numerical, or a combination of both.\n",
      "3. **Optional: Edge Features**: Additional vectors representing the attributes or features of edges in the graph. These features can provide additional context about the connections between nodes.\n",
      "\n",
      "The input to GNNs is usually represented as a triplet:\n",
      "\n",
      "* V (node set): The set of all nodes in the graph\n",
      "* A (edge set): The set of all edges in the graph, represented by their adjacency matrix\n",
      "* X (node feature set): One or more vectors representing the attributes or features of each node\n",
      "\n",
      "The GNN architecture typically operates on this input to extract relevant information about the graph structure and node relationships.\n"
     ]
    }
   ],
   "source": [
    "compare_rag_llm(\"What graph neural networks take as input?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58d435824ed0a744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:08:57.195786Z",
     "start_time": "2025-01-20T19:08:38.612678Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer RAG:\n",
      "I don't know the answer to this question based on the provided context. The text discusses various Graph Neural Network (GNN) architectures, but it does not explicitly define or distinguish between \"dynamic\" and \"static\" graphs. It mentions that some GNNs can handle partial relationship information, but I couldn't find any direct explanation of the difference between dynamic and static graphs.\n",
      "Answer LLM:\n",
      "A dynamic graph and a static graph are two types of graph data structures that differ in how they store and manage their vertices (nodes) and edges.\n",
      "\n",
      "**Static Graph:**\n",
      "\n",
      "In a static graph, all the vertices and edges are stored in memory at once. When you add or remove vertices or edges, it requires updating the entire graph data structure. This can be inefficient for large graphs, as it involves copying or modifying a significant amount of data.\n",
      "\n",
      "Characteristics of a static graph:\n",
      "\n",
      "* All vertices and edges are stored in memory simultaneously.\n",
      "* Adding or removing vertices or edges requires updating the entire graph data structure.\n",
      "* Can be less efficient for large graphs due to memory usage and update overhead.\n",
      "\n",
      "**Dynamic Graph:**\n",
      "\n",
      "In a dynamic graph, only the necessary vertices and edges are stored in memory at any given time. When you add or remove vertices or edges, it involves modifying the graph data structure incrementally, without copying or updating the entire graph. This approach is more efficient for large graphs, as it minimizes memory usage and update overhead.\n",
      "\n",
      "Characteristics of a dynamic graph:\n",
      "\n",
      "* Only necessary vertices and edges are stored in memory at any given time.\n",
      "* Adding or removing vertices or edges involves modifying the graph data structure incrementally.\n",
      "* More efficient for large graphs due to reduced memory usage and update overhead.\n",
      "\n",
      "**Advantages of Dynamic Graphs:**\n",
      "\n",
      "1. Reduced memory usage: Only the necessary vertices and edges are stored, which can lead to significant memory savings.\n",
      "2. Improved performance: Incremental updates reduce the overhead of updating the entire graph data structure.\n",
      "3. Better scalability: Dynamic graphs can handle large datasets more efficiently than static graphs.\n",
      "\n",
      "**Disadvantages of Dynamic Graphs:**\n",
      "\n",
      "1. Increased complexity: Managing dynamic graphs requires additional logic and data structures to keep track of changes.\n",
      "2. Potential performance impact: If not implemented carefully, incremental updates can lead to slower performance.\n",
      "\n",
      "In summary, the choice between a dynamic graph and a static graph depends on the specific use case and requirements. Static graphs are suitable for situations where memory usage is not a concern, while dynamic graphs offer better scalability and efficiency for large datasets or applications with frequent insertions or deletions.\n"
     ]
    }
   ],
   "source": [
    "compare_rag_llm(\"What is the difference between dynamic and static graph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a45a4454e6f36feb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:09:24.955897Z",
     "start_time": "2025-01-20T19:08:57.197728Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer RAG:\n",
      "Unfortunately, I don't know how ST-GCN (Spatial-Temporal Graph Convolutional Network) works, as it is not explicitly described in the provided text. The text only provides a general overview of Spatial-Temporal Graph Neural Networks (STGNNs) and mentions that ST-GCN alleviates the sequential nature of PolygonRNN by predicting all vertices simultaneously using a Graph Convolutional Network (GCN), but it does not provide further details on how this is implemented.\n",
      "Answer LLM:\n",
      "ST-GCN (Spatial-Temporal Graph Convolutional Network) is a type of neural network architecture that combines spatial and temporal graph convolutional layers to process data on graphs. Here's an overview of how it works:\n",
      "\n",
      "**Architecture**\n",
      "\n",
      "The ST-GCN architecture consists of two main components:\n",
      "\n",
      "1. **Spatial Graph Convolution Layer**: This layer applies graph convolution operations to the input graph, treating nodes as independent features.\n",
      "2. **Temporal Graph Convolution Layer**: This layer applies graph convolution operations to the input graph, taking into account the temporal relationships between nodes.\n",
      "\n",
      "**Spatial Graph Convolution Layer**\n",
      "\n",
      "The spatial graph convolution layer uses a standard graph convolutional network (GCN) architecture to process the input graph. The GCN layer takes in an adjacency matrix A and an embedding vector x_i for each node i as input. The output of the GCN layer is computed using the following formula:\n",
      "\n",
      "h_i = sum_{j ∈ N(i)} \\sigma(W_Ax_j + b)\n",
      "\n",
      "where h_i represents the feature vector of node i, W_A is a learnable matrix, and σ is an activation function (e.g., ReLU).\n",
      "\n",
      "**Temporal Graph Convolution Layer**\n",
      "\n",
      "The temporal graph convolution layer extends the spatial GCN layer to incorporate temporal relationships between nodes. It uses a recurrent neural network (RNN) component to model the temporal dependencies.\n",
      "\n",
      "Specifically, the temporal graph convolution layer takes in the adjacency matrix A and an embedding vector x_i for each node i as input. The output of this layer is computed using the following formula:\n",
      "\n",
      "h_i = sum_{j ∈ N(i)} \\sigma(W_Ax_j + U) + tanh(tanh(W_h(h_j)))\n",
      "\n",
      "where h_i represents the feature vector of node i, W_A and U are learnable matrices, σ is an activation function (e.g., ReLU), W_h is a learnable matrix that maps the previous hidden state to the current hidden state, and tanh is the hyperbolic tangent function.\n",
      "\n",
      "**Combining Spatial and Temporal Layers**\n",
      "\n",
      "The final output of the ST-GCN architecture is obtained by combining the outputs of both spatial and temporal graph convolution layers. Specifically, the output is computed using the following formula:\n",
      "\n",
      "h = [h_1; h_2; ...; h_N]\n",
      "\n",
      "where h represents the global representation of the entire graph.\n",
      "\n",
      "**Training**\n",
      "\n",
      "During training, the ST-GCN model learns to predict a set of labels for each node in the graph by minimizing a loss function (e.g., cross-entropy) between the predicted labels and the actual labels. The model is optimized using an optimization algorithm such as stochastic gradient descent (SGD).\n",
      "\n",
      "**Applications**\n",
      "\n",
      "ST-GCN has been applied to various applications, including:\n",
      "\n",
      "1. **Graph-based recommendation systems**: ST-GCN can be used to build graph-based recommendation systems that take into account both spatial and temporal relationships between nodes.\n",
      "2. **Network analysis**: ST-GCN can be used for network analysis tasks such as node classification, clustering, and anomaly detection.\n",
      "3. **Traffic flow prediction**: ST-GCN can be used to predict traffic flows on road networks by incorporating spatial and temporal information.\n",
      "\n",
      "In summary, the ST-GCN architecture combines spatial and temporal graph convolutional layers to process data on graphs, which enables it to capture both local and global patterns in the graph structure and temporal dynamics.\n"
     ]
    }
   ],
   "source": [
    "compare_rag_llm(\"Explain how does ST- GCN works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afeba83ac3c00789",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:10:01.638495Z",
     "start_time": "2025-01-20T19:09:24.959385Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer RAG:\n",
      "Yes, we can use Graph Neural Networks (GNNs) in Computer Vision. According to the context, GNNs are a family of graph networks inspired by mechanisms existing between nodes on a graph, and they have been increasingly used in computer vision for various applications such as:\n",
      "\n",
      "* Biometrics\n",
      "* Face recognition\n",
      "* Gesture recognition\n",
      "* Body pose recognition\n",
      "* Video analysis and understanding\n",
      "* Action and behavior recognition\n",
      "* Computational photography\n",
      "* Image and video synthesis from zero or few shots\n",
      "\n",
      "GNNs can help express and provide a more explainable representation of data, making them a useful tool in computer vision. The paper being discussed aims to collect papers on GNN-based approaches towards computer vision, highlighting their architectures, datasets, and common approaches.\n",
      "\n",
      "However, the question \"How?\" is not explicitly answered in the provided context. If you're looking for specific techniques or methods used to apply GNNs in computer vision, it would require further information or research beyond this abstract.\n",
      "Answer LLM:\n",
      "Yes, Graph Neural Networks (GNNs) can be used in computer vision. In fact, GNNs have shown promise in various computer vision tasks such as image segmentation, object detection, and visual question answering.\n",
      "\n",
      "Here are some ways to apply GNNs in computer vision:\n",
      "\n",
      "1. **Graph-based Object Representation**: Instead of using traditional Euclidean distance metrics for object representation, GNNs can be used to learn graph-based representations that capture the spatial relationships between pixels or features in an image.\n",
      "2. **Scene Graphs**: GNNs can be used to construct and reason about scene graphs, which represent objects, their interactions, and the layout of a scene.\n",
      "3. **Image Segmentation**: GNNs can be used for image segmentation by modeling the spatial relationships between pixels as a graph, where each node represents a pixel or region of interest.\n",
      "4. **Object Detection**: GNNs can be used for object detection by modeling the spatial relationships between objects and their bounding boxes in an image.\n",
      "5. **Visual Question Answering**: GNNs can be used to answer visual questions about an image by modeling the relationships between objects, regions, and question words.\n",
      "\n",
      "Some common graph neural network architectures used in computer vision include:\n",
      "\n",
      "1. **Graph Convolutional Networks (GCNs)**: GCNs are a type of GNN that use convolutional layers to process node features.\n",
      "2. **Gated Graph Convolutional Networks (GGCNs)**: GGCNs are an extension of GCNs that add a gate mechanism to control the flow of information between nodes.\n",
      "3. **Graph Attention Networks (GANs)**: GANs are a type of GNN that use attention mechanisms to weigh the importance of edges in the graph.\n",
      "\n",
      "Some popular applications of GNNs in computer vision include:\n",
      "\n",
      "1. **DeepGraph**: DeepGraph is a neural network architecture that uses GNNs for image segmentation and object detection.\n",
      "2. **Visual Graph Neural Networks (VGNNs)**: VGNNs are a family of GNN architectures that have been applied to various computer vision tasks, including image segmentation and object detection.\n",
      "3. **Graph-based Visual Question Answering (G-VQA)**: G-VQA is a system that uses GNNs for visual question answering.\n",
      "\n",
      "Some key benefits of using GNNs in computer vision include:\n",
      "\n",
      "1. **Improved accuracy**: GNNs can capture complex spatial relationships between objects, leading to improved performance on certain tasks.\n",
      "2. **Robustness to changes**: GNNs can be more robust to changes in the input data, such as partial occlusions or lighting variations.\n",
      "3. **Flexibility**: GNNs can be applied to a wide range of computer vision tasks and can be easily adapted to new tasks.\n",
      "\n",
      "However, GNNs also have some limitations and challenges, including:\n",
      "\n",
      "1. **Computational complexity**: Training GNNs can be computationally expensive due to the need to propagate information through large graphs.\n",
      "2. **Data requirements**: GNNs typically require large amounts of labeled data to train effectively.\n",
      "3. **Interpretability**: GNNs can be difficult to interpret, making it challenging to understand why a particular prediction was made.\n",
      "\n",
      "In summary, GNNs have shown promise in computer vision and can be used for various tasks such as image segmentation, object detection, and visual question answering. However, their use is not without limitations and challenges, and careful consideration must be given to the computational complexity, data requirements, and interpretability of these models.\n"
     ]
    }
   ],
   "source": [
    "compare_rag_llm(\"Can we use Graph Neural Network in Computer Vision? How?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2b7207e59dc35db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T19:10:22.350302Z",
     "start_time": "2025-01-20T19:10:01.644131Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer RAG:\n",
      "The text provides a list of the most commonly used benchmark datasets for testing Graph Neural Networks (GNNs) in computer vision, as presented in Table I. The datasets mentioned include:\n",
      "\n",
      "1. NTU-RGBD - Action and behavior recognition\n",
      "2. CUHK03 - Biometrics, face, gesture, body pose\n",
      "3. Skeleton-Kinetics - Action and behavior recognition\n",
      "4. Visual Genome (Scene analysis and Low-level and physics-based vision)\n",
      "5. ShanghaiTech - Video analysis and understanding\n",
      "6. COCO (Recognition) - Object detection and categorization\n",
      "7. MS-Caleb-1M - Computational photography, image and video synthesis\n",
      "\n",
      "These datasets are commonly used in GNN-based computer vision studies.\n",
      "Answer LLM:\n",
      "Most Common Benchmark Datasets for GNN:\n",
      "\n",
      "1. **Cora**: A graph classification dataset consisting of 2708 nodes and 17 features. It is widely used as a benchmark for evaluating the performance of Graph Neural Networks (GNNs).\n",
      "2. **CiteSeer**: Similar to Cora, but with more nodes (2660) and edges (1664). It is also commonly used for GNN evaluation.\n",
      "3. **Chemoinformatics datasets**:\n",
      " * **Proteins 3D (PDB)**: A dataset of protein structures, which can be used to evaluate the performance of GNNs on molecular graph tasks such as node classification and regression.\n",
      " * **MolGraph**: A dataset of chemical compounds with various properties, used for evaluating GNN-based methods in cheminformatics.\n",
      "4. **NetProtein**\n",
      " * This is a dataset composed of protein-protein interaction networks from the Human Protein Interaction Network (HPIN).\n",
      "5. **Omniglot**: A multi-class image classification dataset, where each class corresponds to a different type of handwritten digit or symbol.\n",
      "6. **Reddit Graph**:\n",
      " * This is a large-scale graph dataset containing information about users, posts, and comments on Reddit.com.\n",
      "\n",
      "These benchmark datasets are widely used in the GNN community for evaluating the performance of various GNN architectures, such as GCN, GraphSAGE, and Graph Convolutional Networks (GCNs).\n"
     ]
    }
   ],
   "source": [
    "compare_rag_llm(\"What are most common benchmark datasets to test GNN on?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a26287f0d5298",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Wnioski\n",
    "Sam LLM (Ollama) w moim przypadku ma tendencje do znacznie dlużyszch odpowiedzi. Niekiedy może być to na plus, niekiedy na minus. Podobnie jak odpowiadanie na pytania których nie ma lub częściowo nie ma w bazie danych. RAG, nie odpowiada i informuje nas o tym, że brak mu info, jednak LLM praktycznie zawsze odpowie, nie musimy się tutaj obawiac brakiem informacjiw  bazie jednak narażamy się na halucynacje. Niemniej, był to RAG robiony dla zadania QA, co za tym idzie myśle, że częsciej be∂zie nas interesowało QA z jakimś konktekstem i wykorzysytwaniem ograniczonych informacji, w tym celu RAG wydaje się być lepszą opcją. Mamy tutaj dostęp i odpowiedzi na pytania z naszej bazy, jesli ich nie ma to też jestesmy o tym informowani. Jeśli chcielibyśmy zadwać pytania na różne tematy, coś ala Open-Domain QA, penwie skorzystamy z gotowych rozwiązań (Ogromnych LLM a nie naszych mneijszych) ew. wyszukiwarki google po prostu a nie będziemy implementować RAG. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef6292710318",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Questions\n",
    "- How does RAG improve the quality and reliability of LLM responses compared to pure LLM generation?\n",
    "    RAG odpowiadał tylko na pytania do których miał jasne odpowiedzi w texcie, odpowaidał dobrze i na bazei kontekstu z bazy danych.\n",
    "- What are the key factors affecting RAG performance (chunk size, embedding quality, prompt design)?\n",
    "    - wiekszy chunk wiekszy kontekts, jednak dluzej zajmuje przetworzenie, ponadto moze utrudniac to zwiezle i krotkie odpowiedzi\n",
    "    - Im lepsze embeddingi tym lepsza separacja danych i rozroznienie, dobre embeddingi zawsze na plus\n",
    "    - prompt design to do ostatniej części - LLM, im lepszy i dokładniejszy prompt model zwróci nam odpowiedz we formacie jaki chcemy i to co chcemy.\n",
    "- How does the choice of vector database and embedding model impact system performance?\n",
    "    - Baza danych odpowiada głównie za wydajność (szybkość) działania, embeddingi za jakość (jak separowalne lub nie są dane)\n",
    "- What are the main challenges in implementing a production-ready RAG system?\n",
    "    - Budowanie dużej bazy danych, indexowanie jej, i używanie embeddingow które najlepiej pasują do naszego use-case\n",
    "- How can the system be improved to handle complex queries requiring multiple document lookups\n",
    "    - Mozemy pozowlic systemowi aby wyciagal kilka dokumentow, nastepnie laczyc je w jeden duzy kontekst i dopiero z tego duzego kontekstu poprosic o ostateczna odpowiedz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4458991ea47e8bf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
